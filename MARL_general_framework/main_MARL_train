# Born time: 2022-5-10
# Latest update: 2022-5-12
# RL Training Phase
# Hengxi
import itertools

import gym
from agent_marl import Agent_GU, Agent_UAV, ReplayMemory_UAV, ReplayMemory_GU
import numpy as np
import random
import torch
import torch.nn as nn

from env_SAGI import Env_SAGI

MIN_REPLAY_SIZE = 1000
BUFFER_SIZE = 500000

# env = gym.make("CartPole-v1")

n_agent_GU = 100
n_agent_UAV = 50
n_action_GUs = 3
n_action_UAVs = 3

a_GUs = np.random.randint(low=0, high=4, size=(n_agent_GU, n_action_GUs))
a_UAVs = np.random.randint(low=0, high=4, size=(n_agent_UAV, n_action_UAVs))

env = Env_SAGI(n_agent_GU, n_agent_UAV, a_GUs, a_UAVs)

current_obs_GUs, current_obs_UAVs = env.get_state()
dim_obs_GUs = len(current_obs_GUs.T)
dim_obs_UAVs = len(current_obs_UAVs.T)

"""Generate GU and UAV agents"""
agent_GUs = []
for i in range(n_agent_GU):
    GU_i = Agent_GU(idx=i,
                    n_input=dim_obs_GUs,
                    n_output=n_action_GUs,
                    n_UAV=n_agent_UAV,
                    mode='train')
    agent_GUs.append(GU_i)

agent_UAVs = []
for j in range(n_agent_UAV):
    UAV_j = Agent_UAV(idx=j,
                      n_input=dim_obs_UAVs,
                      n_output=n_action_UAVs,
                      mode='train')
    agent_UAVs.append(UAV_j)

IS_TRAIN = 1
IS_TEST = 1 - IS_TRAIN

"""Main training loop"""
if IS_TRAIN:

    n_episode = 3000
    n_time_step = 1000

    EPSILON_START = 1.0
    EPSILON_END = 0.02
    EPSILON_DECAY = 10000

    # REWARD_BUFFER = np.empty(shape=n_episode)
    REWARD_BUFFER = []

    a_GUs = np.empty(shape=(n_agent_GU, n_action_GUs))  # to initialize the action space of all
    a_UAVs = np.empty(shape=(n_agent_UAV, n_action_UAVs))
    # for episode_i in range(n_episode):
    for episode_i in itertools.count():
        """Action selection"""
        epsilon = np.interp(episode_i, [0, EPSILON_DECAY],
                            [EPSILON_START, EPSILON_END])  # epsilon annealing interpolation
        reward_per_episode = 0
        for time_step_i in range(n_time_step):
            random_sample = random.random()
            if random_sample <= epsilon:  # randomly choose actions
                # GUs
                for i in range(n_agent_GU):
                    a_GUs[i] = [np.random.choice(k) for k in agent_GUs[i].action_space]  # joint action vector
                # UAVs
                for j in range(n_agent_UAV):
                    a_UAVs[j] = [np.random.choice(k) for k in agent_UAVs[j].action_space]
            else:
                # GUs
                for i in range(n_agent_GU):  # choose actions using nn
                    a_GUs[i] = agent_GUs[i].online_net.act(current_obs_GUs)
                # UAVs
                for j in range(n_agent_UAV):
                    a_UAVs[j] = agent_UAVs[j].online_net.act(current_obs_UAVs)

            """Env transition"""
            next_obs_GUs, next_obs_UAVs, r = env.step(a_GUs, a_UAVs)  # same reward: cooperative game
            env.update()

            """Agents updating memories"""
            for i in range(n_agent_GU):
                agent_GUs[i].memo.add_memo(current_obs_GUs[i], a_GUs[i], r, next_obs_GUs[i])

            for j in range(n_agent_UAV):
                agent_UAVs[j].memo.add_memo(current_obs_UAVs[j], a_UAVs[j], r, next_obs_UAVs[j])

            current_obs_GUs = next_obs_GUs
            current_obs_UAVs = next_obs_UAVs
            reward_per_episode += r

        # REWARD_BUFFER[episode_i] = reward_per_episode
        REWARD_BUFFER.append(reward_per_episode)

        """Start Gradient Step"""
        """GU"""
        # GUs batch storage space
        batch_current_obs_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE, dim_obs_GUs))
        batch_a_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE, n_action_GUs))
        batch_r = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE))
        batch_next_obs_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE, dim_obs_GUs))
        max_target_q_values_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE))
        q_targets_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE))
        q_predicts_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE, n_action_GUs))
        a_q_values_GUs = np.empty(shape=(n_agent_GU, agent_GUs[0].BATCH_SIZE))
        loss_GUs = np.empty(shape=n_agent_GU)

        for i in range(n_agent_GU):

            """Experience replay"""
            batch_current_obs_GUs[i], batch_a_GUs[i], batch_r[i], batch_next_obs_GUs[i] = agent_GUs[
                i].memo.sample()  # update batch-size amounts of Q

            """Compute Targets"""
            max_target_q_values_GUs[i] = agent_GUs[i].target_net(batch_next_obs_GUs).max(dim=1, keepdim=True)[i][0]
            q_targets_GUs[i] = batch_r[i] + agent_GUs[i].GAMMA * max_target_q_values_GUs[i]

            """Compute Loss"""
            q_predicts_GUs[i] = agent_GUs[i].online_net(batch_current_obs_GUs)

            # TODO dimension: 100x4x10
            a_q_values_GUs[i] = torch.gather(input=q_predicts_GUs[i], dim=1, index=batch_a_GUs)  # ?

            loss_GUs[i] = nn.functional.smooth_l1_loss(a_q_values_GUs[i], q_targets_GUs[i])

            """Gradient Descent"""
            agent_GUs[i].optimizer.zero_grad()
            loss_GUs[i].backward()
            agent_GUs[i].optimizer.step()

            """Update target network"""
            if time_step_i % agent_GUs[i].TARGET_UPDATE_FREQUENCY == 0:
                agent_GUs[i].target_net.load_state_dict(agent_GUs[i].online_net.state_dict())  # ?

        """UAV"""
        # UAVs batch storage space
        batch_current_obs_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE, dim_obs_UAVs))
        batch_a_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE, n_action_UAVs))
        batch_r = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE))
        batch_next_obs_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE, dim_obs_UAVs))
        max_target_q_values_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE))
        q_targets_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE))
        q_predicts_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE, n_action_UAVs))
        a_q_values_UAVs = np.empty(shape=(n_agent_UAV, agent_UAVs[0].BATCH_SIZE))
        loss_UAVs = np.empty(shape=n_agent_UAV)

        for i in range(n_agent_UAV):

            """Experience replay"""
            batch_current_obs_UAVs[i], batch_a_UAVs[i], batch_r[i], batch_next_obs_UAVs[i] = agent_UAVs[
                i].memo.sample()  # update batch-size amounts of Q

            """Compute Targets"""
            max_target_q_values_UAVs[i] = agent_UAVs[i].target_net(batch_next_obs_UAVs).max(dim=1, keepdim=True)[i][0]
            q_targets_UAVs[i] = batch_r[i] + agent_UAVs[i].GAMMA * max_target_q_values_UAVs[i]

            """Compute Loss"""
            q_predicts_UAVs[i] = agent_UAVs[i].online_net(batch_current_obs_UAVs)

            # TODO dimension: 100x4x10
            a_q_values_UAVs[i] = torch.gather(input=q_predicts_UAVs[i], dim=1, index=batch_a_UAVs)  # ?

            loss_UAVs[i] = nn.functional.smooth_l1_loss(a_q_values_UAVs[i], q_targets_UAVs[i])

            """Gradient Descent"""
            agent_UAVs[i].optimizer.zero_grad()
            loss_UAVs[i].backward()
            agent_UAVs[i].optimizer.step()

            """Update target network"""
            if time_step_i % agent_UAVs[i].TARGET_UPDATE_FREQUENCY == 0:
                agent_UAVs[i].target_net.load_state_dict(agent_UAVs[i].online_net.state_dict())  # ?

        """Print the training progress"""
        print("Step: {}".format(episode_i))
        print("Avg reward: {}".format(np.mean(REWARD_BUFFER)))
